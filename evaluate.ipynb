{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "\n",
    "from subword_prompt_templates.MultiChoicePrompts import MultiChoicePrompts\n",
    "from subword_prompt_templates.ClassificationPrompts import ClassificationPrompts\n",
    "\n",
    "from utils import helpers\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['MISTRALAI/MIXTRAL-8X7B-INSTRUCT-V0.1', 'allenai/OLMo-7B-Instruct', 'meta-llama/Llama-3-8b-chat-hf']\n",
    "prompts = ['zero_shot', 'one_shot', 'few_shot', 'cot\\one', 'cot\\few', 'decomposite']\n",
    "dataset = pd.read_csv(r\"datasets\\dataset_for_evaluation.csv\", index_col=0)\n",
    "\n",
    "TOGETHER_API_KEY = os.environ.get(\"TOGETHER_API_KEY\")\n",
    "client = OpenAI(api_key=TOGETHER_API_KEY, base_url='https://api.together.xyz/v1')\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-chioce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Prompt Type</th>\n",
       "      <th>Question type</th>\n",
       "      <th>Question</th>\n",
       "      <th>LLM Response</th>\n",
       "      <th>Correct answer</th>\n",
       "      <th>Eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MISTRALAI/MIXTRAL-8X7B-INSTRUCT-V0.1</td>\n",
       "      <td>zero_shot</td>\n",
       "      <td>multi</td>\n",
       "      <td>Subword is a standalone word that exists withi...</td>\n",
       "      <td>A. Incredible - The subword \"red\" can be foun...</td>\n",
       "      <td>A</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISTRALAI/MIXTRAL-8X7B-INSTRUCT-V0.1</td>\n",
       "      <td>zero_shot</td>\n",
       "      <td>classification</td>\n",
       "      <td>Subword is a standalone word that exists withi...</td>\n",
       "      <td>Yes, the word \"incredible\" contains the subwo...</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MISTRALAI/MIXTRAL-8X7B-INSTRUCT-V0.1</td>\n",
       "      <td>zero_shot</td>\n",
       "      <td>classification</td>\n",
       "      <td>Subword is a standalone word that exists withi...</td>\n",
       "      <td>Yes, the word \"lower\" contains the subword \"r...</td>\n",
       "      <td>No</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Model Prompt Type   Question type  \\\n",
       "0  MISTRALAI/MIXTRAL-8X7B-INSTRUCT-V0.1   zero_shot           multi   \n",
       "1  MISTRALAI/MIXTRAL-8X7B-INSTRUCT-V0.1   zero_shot  classification   \n",
       "2  MISTRALAI/MIXTRAL-8X7B-INSTRUCT-V0.1   zero_shot  classification   \n",
       "\n",
       "                                            Question  \\\n",
       "0  Subword is a standalone word that exists withi...   \n",
       "1  Subword is a standalone word that exists withi...   \n",
       "2  Subword is a standalone word that exists withi...   \n",
       "\n",
       "                                        LLM Response Correct answer   Eval  \n",
       "0   A. Incredible - The subword \"red\" can be foun...              A   True  \n",
       "1   Yes, the word \"incredible\" contains the subwo...            yes   True  \n",
       "2   Yes, the word \"lower\" contains the subword \"r...             No  False  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "mcp = MultiChoicePrompts()\n",
    "ynp = ClassificationPrompts()\n",
    "for model in models:\n",
    "    for prompt_type in prompts:\n",
    "        for row in dataset.itertuples():\n",
    "            wrong_choices = json.loads(row.Mulitple_Options.replace(\" \", \", \").replace(\"'\", '\"'))\n",
    "            correct_choice = row.Word\n",
    "            choices = wrong_choices + [correct_choice]\n",
    "            random.shuffle(choices)\n",
    "            correct_choice_letter = chr(choices.index(correct_choice) + 65)\n",
    "            multi_question = mcp.generate(prompt_type, row.Category, *choices)\n",
    "            yes_question = ynp.generate(prompt_type, row.Category, row.Word)\n",
    "            no_question = ynp.generate(prompt_type, row.Category, random.choice(wrong_choices))\n",
    "            multi_answer = helpers.generate_llm_response(client, multi_question, model)\n",
    "            yes_answer = helpers.generate_llm_response(client, yes_question, model)\n",
    "            no_answer = helpers.generate_llm_response(client, no_question, model)\n",
    "            multi_eval = helpers.evaluate_response('multi', multi_answer, correct_choice_letter)\n",
    "            yes_eval = helpers.evaluate_response('classification', yes_answer, \"Yes\")\n",
    "            no_eval = helpers.evaluate_response('classification', no_answer, \"No\")\n",
    "            results.append((model, prompt_type, 'multi', multi_question, multi_answer, correct_choice_letter, multi_eval))\n",
    "            results.append((model, prompt_type, 'classification', yes_question, yes_answer, \"yes\", yes_eval))\n",
    "            results.append((model, prompt_type, 'classification', no_question, no_answer, \"No\", no_eval))\n",
    "            break\n",
    "        break\n",
    "    break\n",
    "pd.DataFrame(results, columns=[\"Model\", \"Prompt Type\", \"Question type\", \"Question\", \"LLM Response\", \"Correct answer\", \"Eval\"])#.to_csv(\"response_list.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yes\\No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'A.' starts with 'A'\n",
      "'A)' starts with 'A'\n",
      "'[A]' starts with 'A'\n",
      "'A' starts with 'A'\n",
      "'An' does not match the pattern.\n",
      "'A+' starts with 'A'\n",
      "'A*' starts with 'A'\n",
      "'A ' starts with 'A'\n",
      "' A ' starts with 'A'\n",
      "'The A' does not match the pattern.\n",
      "'A is the correct answer' starts with 'A'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "gold_response = \"A\"\n",
    "\n",
    "pattern = rf'\\W*{re.escape(gold_response)}\\W*\\b'\n",
    "\n",
    "# Example strings to test\n",
    "test_strings = [\"A.\", \"A)\", \"[A]\", \"A\", \"An\", \"A+\", \"A*\", \"A \", \" A \", \"The A\", \"A is the correct answer\"]\n",
    "\n",
    "# Check each string\n",
    "for string in test_strings:\n",
    "    if re.match(pattern, string):\n",
    "        print(f\"'{string}' starts with '{gold_response}'\")\n",
    "    else:\n",
    "        print(f\"'{string}' does not match the pattern.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Yes.' starts with 'Yes'\n",
      "'Yes)' starts with 'Yes'\n",
      "'[Yes]' starts with 'Yes'\n",
      "'Yes' starts with 'Yes'\n",
      "'Yess' does not match the pattern.\n",
      "'Yes ' starts with 'Yes'\n",
      "' Yes ' starts with 'Yes'\n",
      "'The Yes' does not match the pattern.\n",
      "'Yes is the correct answer' starts with 'Yes'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "gold_response = \"Yes\"\n",
    "\n",
    "pattern = rf'\\W*{gold_response}\\b'\n",
    "\n",
    "# Example strings to test\n",
    "test_strings = [\"Yes.\", \"Yes)\", \"[Yes]\", \"Yes\", \"Yess\", \"Yes \", \" Yes \", \"The Yes\", \"Yes is the correct answer\"]\n",
    "\n",
    "# Check each string\n",
    "for string in test_strings:\n",
    "    if re.match(pattern, string):\n",
    "        print(f\"'{string}' starts with '{gold_response}'\")\n",
    "    else:\n",
    "        print(f\"'{string}' does not match the pattern.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
