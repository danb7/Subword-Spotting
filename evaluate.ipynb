{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "\n",
    "from subword_prompt_templates.MultiChoicePrompts import MultiChoicePrompts\n",
    "from subword_prompt_templates.ClassificationPrompts import ClassificationPrompts\n",
    "\n",
    "from utils import helpers\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['MISTRALAI/MIXTRAL-8X7B-INSTRUCT-V0.1', 'allenai/OLMo-7B-Instruct', 'meta-llama/Llama-3-8b-chat-hf']\n",
    "prompts = ['zero_shot', 'one_shot', 'few_shot', 'CoT-one', 'CoT-few', 'decomposite']\n",
    "dataset = pd.read_csv(r\"datasets\\dataset_for_evaluation.csv\", index_col=0)\n",
    "\n",
    "TOGETHER_API_KEY = os.environ.get(\"TOGETHER_API_KEY\")\n",
    "client = OpenAI(api_key=TOGETHER_API_KEY, base_url='https://api.together.xyz/v1')\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "mcp = MultiChoicePrompts()\n",
    "ynp = ClassificationPrompts()\n",
    "for model in models:\n",
    "    for prompt_type in prompts:\n",
    "        if '-' in prompt_type: # for CoT\n",
    "            prompt_technique, shot = prompt_type.split('-')\n",
    "            kwargs = {'shot': shot}\n",
    "        else:\n",
    "            prompt_technique = prompt_type\n",
    "            kwargs = {}\n",
    "        for row in dataset[:12].itertuples():\n",
    "            wrong_choices = json.loads(row.Mulitple_Options.replace(\" \", \", \").replace(\"'\", '\"'))\n",
    "            correct_choice = row.Word\n",
    "            choices = wrong_choices + [correct_choice]\n",
    "            random.shuffle(choices)\n",
    "            correct_choice_letter = chr(choices.index(correct_choice) + 65)\n",
    "            multi_question = mcp.generate(prompt_technique, row.Category, *choices, **kwargs)\n",
    "            yes_question = ynp.generate(prompt_technique, row.Category, row.Word, **kwargs)\n",
    "            no_question = ynp.generate(prompt_technique, row.Category, random.choice(wrong_choices), **kwargs)\n",
    "            multi_answer = helpers.generate_llm_response(client, multi_question, model)\n",
    "            yes_answer = helpers.generate_llm_response(client, yes_question, model)\n",
    "            no_answer = helpers.generate_llm_response(client, no_question, model)\n",
    "            multi_match = helpers.match_answer(multi_answer)\n",
    "            yes_match = helpers.match_answer(yes_answer)\n",
    "            no_match = helpers.match_answer(no_answer)\n",
    "            multi_eval = helpers.evaluate_response(multi_answer, correct_choice_letter)\n",
    "            yes_eval = helpers.evaluate_response(yes_answer, \"Yes\")\n",
    "            no_eval = helpers.evaluate_response(no_answer, \"No\")\n",
    "            results.append((model, prompt_type, 'multi', multi_question, multi_answer, multi_match, correct_choice_letter, multi_eval))\n",
    "            results.append((model, prompt_type, 'classification', yes_question, yes_answer, yes_match, \"yes\", yes_eval))\n",
    "            results.append((model, prompt_type, 'classification', no_question, no_answer, no_match, \"No\", no_eval))\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"Prompt Type\", \"Question type\", \"Question\", \"LLM Response\", \"LLM match\", \"Correct answer\", \"Eval\"])\n",
    "results_df.to_csv(\"check_evaluate.csv\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-choice results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_pv = results_df[results_df['Question type']=='multi'].pivot_table(index='Model', columns='Prompt Type', values='Eval', aggfunc='mean')[prompts]\n",
    "display(multi_pv)\n",
    "helpers.plot_results(multi_pv, radnom_line=0.25) # for now the lines are not really correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yes\\No results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_pv = results_df[results_df['Question type']=='classification'].pivot_table(index='Model', columns='Prompt Type', values='Eval', aggfunc='mean')[prompts]\n",
    "display(classification_pv)\n",
    "helpers.plot_results(classification_pv, radnom_line=0.5) # for now the lines are not really correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "gold_response = \"A\"\n",
    "\n",
    "pattern = rf'\\W*({re.escape(gold_response)})\\W*\\b'\n",
    "\n",
    "full_pattern = rf'{pattern}|The correct .* (is|are){pattern}'\n",
    "\n",
    "# Example strings to test\n",
    "test_strings = [\"A.\", \"A)\", \"[A]\", \"A\", \"An\", \"A+\", \"A*\", \"A \", \" A \", \"The A\", \"A is the correct answer\",\n",
    "                \"AThe correct answer is (A)\", \"The correct letter is A)\",\n",
    "                '''The correct letter and word are:\n",
    "[A] incredible\n",
    "''',\":\\n[A\", 'The correct letter and word are A. million']\n",
    "\n",
    "# Check each string\n",
    "for string in test_strings:\n",
    "    match = re.match(full_pattern, string)\n",
    "    if match:\n",
    "        chosen_letter = [m for m in match.groups() if m is not None][-1]\n",
    "        print(f\"'{string}' starts with '{gold_response}'\", f' - {chosen_letter}')\n",
    "    else:\n",
    "        print(f\"'{string}' does not match the pattern.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "gold_response = \"Yes\"\n",
    "\n",
    "pattern = rf'\\W*{gold_response}\\b'\n",
    "\n",
    "# Example strings to test\n",
    "test_strings = [\"Yes.\", \"Yes)\", \"[Yes]\", \"Yes\", \"Yess\", \"Yes \", \" Yes \", \"The Yes\", \"Yes is the correct answer\"]\n",
    "\n",
    "# Check each string\n",
    "for string in test_strings:\n",
    "    if re.match(pattern, string):\n",
    "        print(f\"'{string}' starts with '{gold_response}'\")\n",
    "    else:\n",
    "        print(f\"'{string}' does not match the pattern.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
